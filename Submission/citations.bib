
@inproceedings{truong_unsupervised_2021,
	title = {Unsupervised {Learning} for {Robust} {Fitting}: {A} {Reinforcement} {Learning} {Approach}},
	shorttitle = {Unsupervised {Learning} for {Robust} {Fitting}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Truong_Unsupervised_Learning_for_Robust_Fitting_A_Reinforcement_Learning_Approach_CVPR_2021_paper.html},
	language = {en},
	urldate = {2024-11-12},
	author = {Truong, Giang and Le, Huu and Suter, David and Zhang, Erchuan and Gilani, Syed Zulqarnain},
	year = {2021},
	pages = {10348--10357},
	file = {Full Text PDF:/home/ziegler/Zotero/storage/RCJHU4J3/Truong et al. - 2021 - Unsupervised Learning for Robust Fitting A Reinfo.pdf:application/pdf},
}

@misc{noauthor_tensorboard_nodate,
	title = {{TensorBoard}},
	url = {http://141.57.41.59:6006/?darkMode=true#scalars&runSelectionState=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},
	urldate = {2024-11-03},
	file = {TensorBoard:/home/ziegler/Zotero/storage/T6R9NNW4/141.57.41.59.html:text/html},
}

@article{baronti_primitive_2019,
	title = {Primitive {Shape} {Fitting} in {Point} {Clouds} {Using} the {Bees} {Algorithm}},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/23/5198},
	doi = {10.3390/app9235198},
	abstract = {In this study the problem of ﬁtting shape primitives to point cloud scenes was tackled as a parameter optimisation procedure, and solved using the popular bees algorithm. Tested on three sets of clean and differently blurred point cloud models, the bees algorithm obtained performances comparable to those obtained using the state-of-the-art random sample consensus (RANSAC) method, and superior to those obtained by an evolutionary algorithm. Shape ﬁtting times were compatible with real-time application. The main advantage of the bees algorithm over standard methods is that it doesn’t rely on ad hoc assumptions about the nature of the point cloud model like RANSAC approximation tolerance.},
	language = {en},
	number = {23},
	urldate = {2024-07-09},
	journal = {Applied Sciences},
	author = {Baronti, Luca and Alston, Mark and Mavrakis, Nikos and Ghalamzan E., Amir M. and Castellani, Marco},
	month = nov,
	year = {2019},
	pages = {5198},
	file = {Baronti et al. - 2019 - Primitive Shape Fitting in Point Clouds Using the .pdf:/home/ziegler/Zotero/storage/55C5S5SC/Baronti et al. - 2019 - Primitive Shape Fitting in Point Clouds Using the .pdf:application/pdf},
}

@inproceedings{li_supervised_2019,
	address = {Long Beach, CA, USA},
	title = {Supervised {Fitting} of {Geometric} {Primitives} to {3D} {Point} {Clouds}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-3293-8},
	url = {https://ieeexplore.ieee.org/document/8953681/},
	doi = {10.1109/CVPR.2019.00276},
	abstract = {Fitting geometric primitives to 3D point cloud data bridges a gap between low-level digitized 3D data and highlevel structural information on the underlying 3D shapes. As such, it enables many downstream applications in 3D data processing. For a long time, RANSAC-based methods have been the gold standard for such primitive ﬁtting problems, but they require careful per-input parameter tuning and thus do not scale well for large datasets with diverse shapes. In this work, we introduce Supervised Primitive Fitting Network (SPFN), an end-to-end neural network that can robustly detect a varying number of primitives at different scales without any user control. The network is supervised using ground truth primitive surfaces and primitive membership for the input points. Instead of directly predicting the primitives, our architecture ﬁrst predicts per-point properties and then uses a differential model estimation module to compute the primitive type and parameters. We evaluate our approach on a novel benchmark of ANSI 3D mechanical component models and demonstrate a signiﬁcant improvement over both the state-of-the-art RANSACbased methods and the direct neural prediction.},
	language = {en},
	urldate = {2024-07-08},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Lingxiao and Sung, Minhyuk and Dubrovina, Anastasia and Yi, Li and Guibas, Leonidas J.},
	month = jun,
	year = {2019},
	pages = {2647--2655},
	file = {Li et al. - 2019 - Supervised Fitting of Geometric Primitives to 3D P.pdf:/home/ziegler/Zotero/storage/4M4UIM7G/Li et al. - 2019 - Supervised Fitting of Geometric Primitives to 3D P.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2024-06-11},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/home/ziegler/Zotero/storage/UGGZ2I68/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@article{yun_action-driven_2018,
	title = {Action-{Driven} {Visual} {Object} {Tracking} {With} {Deep} {Reinforcement} {Learning}},
	volume = {29},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8306309/},
	doi = {10.1109/TNNLS.2018.2801826},
	abstract = {In this paper, we propose an efﬁcient visual tracker, which directly captures a bounding box containing the target object in a video by means of sequential actions learned using deep neural networks. The proposed deep neural network to control tracking actions is pretrained using various training video sequences and ﬁne-tuned during actual tracking for online adaptation to a change of target and background. The pretraining is done by utilizing deep reinforcement learning (RL) as well as supervised learning. The use of RL enables even partially labeled data to be successfully utilized for semisupervised learning. Through the evaluation of the object tracking benchmark data set, the proposed tracker is validated to achieve a competitive performance at three times the speed of existing deep networkbased trackers. The fast version of the proposed method, which operates in real time on graphics processing unit, outperforms the state-of-the-art real-time trackers with an accuracy improvement of more than 8\%.},
	language = {en},
	number = {6},
	urldate = {2024-06-11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Yun, Sangdoo and Choi, Jongwon and Yoo, Youngjoon and Yun, Kimin and Choi, Jin Young},
	month = jun,
	year = {2018},
	pages = {2239--2252},
	file = {Yun et al. - 2018 - Action-Driven Visual Object Tracking With Deep Rei.pdf:/home/ziegler/Zotero/storage/MXEN38LU/Yun et al. - 2018 - Action-Driven Visual Object Tracking With Deep Rei.pdf:application/pdf},
}

@inproceedings{mathe_reinforcement_2016,
	address = {Las Vegas, NV, USA},
	title = {Reinforcement {Learning} for {Visual} {Object} {Detection}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780685/},
	doi = {10.1109/CVPR.2016.316},
	abstract = {One of the most widely used strategies for visual object detection is based on exhaustive spatial hypothesis search. While methods like sliding windows have been successful and effective for many years, they are still brute-force, independent of the image content and the visual category being searched. In this paper we present principled sequential models that accumulate evidence collected at a small set of image locations in order to detect visual objects effectively. By formulating sequential search as reinforcement learning of the search policy (including the stopping condition), our fully trainable model can explicitly balance for each class, speciﬁcally, the conﬂicting goals of exploration – sampling more image regions for better accuracy –, and exploitation – stopping the search efﬁciently when sufﬁciently conﬁdent about the target’s location. The methodology is general and applicable to any detector response function. We report encouraging results in the PASCAL VOC 2012 object detection test set showing that the proposed methodology achieves almost two orders of magnitude speed-up over sliding window methods.},
	language = {en},
	urldate = {2024-06-10},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mathe, Stefan and Pirinen, Aleksis and Sminchisescu, Cristian},
	month = jun,
	year = {2016},
	pages = {2894--2902},
	file = {Mathe et al. - 2016 - Reinforcement Learning for Visual Object Detection.pdf:/home/ziegler/Zotero/storage/FK3RJWEU/Mathe et al. - 2016 - Reinforcement Learning for Visual Object Detection.pdf:application/pdf},
}

@inproceedings{halici_object_2018,
	address = {Athens},
	title = {Object {Localization} {Without} {Bounding} {Box} {Information} {Using} {Generative} {Adversarial} {Reinforcement} {Learning}},
	isbn = {978-1-4799-7061-2},
	url = {https://ieeexplore.ieee.org/document/8451788/},
	doi = {10.1109/ICIP.2018.8451788},
	abstract = {Object localization can be deﬁned as the task of ﬁnding the bounding boxes of objects in a scene. Most of the state-of-the-art approaches utilize meticulously handcrafted training datasets. In this work, we are aiming to create a generative adversarial reinforcement learning framework, which can work without having any explicit bounding box information. Instead of relying on bounding boxes, our framework uses tightly cropped object images as training data. Our image localization framework consists of two parts: a reinforcement learning agent (RL agent) and a discriminator. The RL agent takes input scenes and crops them with the objective of creating a tightly cropped object image. The discriminator tries to distinguish whether the image is generated by the RL agent or it comes from a tightly cropped object database. Experiments indicate that it is possible to achieve a promising localization performance without having explicit bounding box data. It can be concluded that generative adversarial reinforcement learning is an important tool in dealing with other learning problems where explicit input/output paired data is not available.},
	language = {en},
	urldate = {2024-06-10},
	booktitle = {2018 25th {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {Halici, Eren and Aydin Alatan, A.},
	month = oct,
	year = {2018},
	pages = {3728--3732},
	file = {Halici und Aydin Alatan - 2018 - Object Localization Without Bounding Box Informati.pdf:/home/ziegler/Zotero/storage/4JUMZVKT/Halici und Aydin Alatan - 2018 - Object Localization Without Bounding Box Informati.pdf:application/pdf},
}

@article{wang_multitask_2019,
	title = {Multitask {Learning} for {Object} {Localization} {With} {Deep} {Reinforcement} {Learning}},
	volume = {11},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2379-8920, 2379-8939},
	url = {https://ieeexplore.ieee.org/document/8570827/},
	doi = {10.1109/TCDS.2018.2885813},
	abstract = {In object localization, methods based on a top-down search strategy that focus on learning a policy have been widely researched. The performance of these methods relies heavily on the policy in question. This paper proposes a deep Q-network (DQN) that employs a multitask learning method to localize classspeciﬁc objects. This DQN agent consists of two parts, an action executor part and a terminal part. The action executor determines the action that the agent should perform and the terminal decides whether the agent has detected the target object. By taking advantage of the capability of feature learning in a multitask method, our method combines these two parts by sharing hidden layers and trains the agent using multitask learning. A detection dataset from the PASCAL visual object classes challenge 2007 was used to evaluate the proposed method, and the results show that it can achieve higher average precision with fewer search steps than similar methods.},
	language = {en},
	number = {4},
	urldate = {2024-06-10},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Wang, Yan and Zhang, Lei and Wang, Lituan and Wang, Zizhou},
	month = dec,
	year = {2019},
	pages = {573--580},
	file = {Wang et al. - 2019 - Multitask Learning for Object Localization With De.pdf:/home/ziegler/Zotero/storage/8PGIE5WT/Wang et al. - 2019 - Multitask Learning for Object Localization With De.pdf:application/pdf},
}

@inproceedings{caicedo_active_2015,
	address = {Santiago, Chile},
	title = {Active {Object} {Localization} with {Deep} {Reinforcement} {Learning}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410643/},
	doi = {10.1109/ICCV.2015.286},
	abstract = {We present an active detection model for localizing objects in scenes. The model is class-speciﬁc and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most speciﬁc location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
	language = {en},
	urldate = {2024-06-10},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Caicedo, Juan C. and Lazebnik, Svetlana},
	month = dec,
	year = {2015},
	pages = {2488--2496},
	file = {Caicedo und Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:/home/ziegler/Zotero/storage/UVIIUNUC/Caicedo und Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:application/pdf},
}

@article{wang_ppg_nodate,
	title = {{PPG} {Reloaded}: {An} {Empirical} {Study} on {What} {Matters} in {Phasic} {Policy} {Gradient}},
	abstract = {In model-free reinforcement learning, recent methods based on a phasic policy gradient (PPG) framework have shown impressive improvements in sample efficiency and zero-shot generalization on the challenging Procgen benchmark. In PPG, two design choices are believed to be the key contributing factors to its superior performance over PPO: the high level of value sample reuse and the low frequency of feature distillation. However, through an extensive empirical study, we unveil that policy regularization and data diversity are what actually matters. In particular, we can achieve the same level of performance with low value sample reuse and frequent feature distillation, as long as the policy regularization strength and data diversity are preserved. In addition, we can maintain the high performance of PPG while reducing the computational cost to a similar level as PPO. Our comprehensive study covers all 16 Procgen games in both sample efficiency and generalization setups. We hope it can advance the understanding of PPG and provide insights for future works.},
	language = {en},
	author = {Wang, Kaixin and Zhou, Daquan and Feng, Jiashi and Mannor, Shie},
	file = {Wang et al. - PPG Reloaded An Empirical Study on What Matters i.pdf:/home/ziegler/Zotero/storage/ML58AP4B/Wang et al. - PPG Reloaded An Empirical Study on What Matters i.pdf:application/pdf},
}

@misc{cobbe_leveraging_2020,
	title = {Leveraging {Procedural} {Generation} to {Benchmark} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1912.01588},
	abstract = {We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efﬁciency and generalization in reinforcement learning. We believe that the community will beneﬁt from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, ﬁnding that larger models signiﬁcantly improve both sample efﬁciency and generalization.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
	month = jul,
	year = {2020},
	note = {arXiv:1912.01588 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_37_nodate,
	title = {The 37 {Implementation} {Details} of {Proximal} {Policy} {Optimization} · {The} {ICLR} {Blog} {Track}},
	url = {https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/},
	urldate = {2024-05-29},
}

@misc{espeholt_impala_2018,
	title = {{IMPALA}: {Scalable} {Distributed} {Deep}-{RL} with {Importance} {Weighted} {Actor}-{Learner} {Architectures}},
	shorttitle = {{IMPALA}},
	url = {http://arxiv.org/abs/1802.01561},
	abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efﬁciently in singlemachine training but also scales to thousands of machines without sacriﬁcing data efﬁciency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach. The source code is publicly available at github.com/deepmind/scalable agent.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
	month = jun,
	year = {2018},
	note = {arXiv:1802.01561 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Espeholt et al. - 2018 - IMPALA Scalable Distributed Deep-RL with Importan.pdf:/home/ziegler/Zotero/storage/H9GGZC4L/Espeholt et al. - 2018 - IMPALA Scalable Distributed Deep-RL with Importan.pdf:application/pdf},
}

@misc{cobbe_phasic_2020,
	title = {Phasic {Policy} {Gradient}},
	url = {http://arxiv.org/abs/2009.04416},
	abstract = {We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modiﬁes traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we ﬁnd that PPG signiﬁcantly improves sample eﬃciency on the challenging Procgen Benchmark.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman, John},
	month = sep,
	year = {2020},
	note = {arXiv:2009.04416 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Cobbe et al. - 2020 - Phasic Policy Gradient.pdf:/home/ziegler/Zotero/storage/AWEM4ZBY/Cobbe et al. - 2020 - Phasic Policy Gradient.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/home/ziegler/Zotero/storage/YXFEZEAJ/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{khadka_evolution-guided_2018,
	title = {Evolution-{Guided} {Policy} {Gradient} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1805.07917},
	abstract = {Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difﬁculties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversiﬁed data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA’s ability of temporal credit assignment with a ﬁtness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL’s ability to leverage gradients for higher sample efﬁciency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL signiﬁcantly outperforms prior DRL and EA methods.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Khadka, Shauharda and Tumer, Kagan},
	month = oct,
	year = {2018},
	note = {arXiv:1805.07917 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr{\textbackslash}'eal, Canada},
	file = {Khadka und Tumer - 2018 - Evolution-Guided Policy Gradient in Reinforcement .pdf:/home/ziegler/Zotero/storage/XLYX5WQY/Khadka und Tumer - 2018 - Evolution-Guided Policy Gradient in Reinforcement .pdf:application/pdf},
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inproceedings{huang_learning_2018,
	address = {Salt Lake City, UT},
	title = {Learning to {Parse} {Wireframes} in {Images} of {Man}-{Made} {Environments}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578170/},
	doi = {10.1109/CVPR.2018.00072},
	abstract = {In this paper, we propose a learning-based approach to the task of automatically extracting a “wireframe” representation for images of cluttered man-made environments. The wireframe (see Fig. 1) contains all salient straight lines and their junctions of the scene that encode efﬁciently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved signiﬁcantly better performance than stateof-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efﬁciently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could beneﬁt many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation. The data and source code are available at https: //github.com/huangkuns/wireframe.},
	language = {en},
	urldate = {2025-01-10},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Huang, Kun and Wang, Yifan and Zhou, Zihan and Ding, Tianjiao and Gao, Shenghua and Ma, Yi},
	month = jun,
	year = {2018},
	pages = {626--635},
	file = {PDF:/home/ziegler/Zotero/storage/BHWAL4Q3/Huang et al. - 2018 - Learning to Parse Wireframes in Images of Man-Made Environments.pdf:application/pdf},
}

@article{xue_holistically-attracted_2023,
	title = {Holistically-{Attracted} {Wireframe} {Parsing}: {From} {Supervised} to {Self}-{Supervised} {Learning}},
	volume = {45},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Holistically-{Attracted} {Wireframe} {Parsing}},
	url = {http://arxiv.org/abs/2210.12971},
	doi = {10.1109/TPAMI.2023.3312749},
	abstract = {This article presents Holistically-Attracted Wireframe Parsing (HAWP), a method for geometric analysis of 2D images containing wireframes formed by line segments and junctions. HAWP utilizes a parsimonious Holistic Attraction (HAT) field representation that encodes line segments using a closed-form 4D geometric vector field. The proposed HAWP consists of three sequential components empowered by end-to-end and HAT-driven designs: (1) generating a dense set of line segments from HAT fields and endpoint proposals from heatmaps, (2) binding the dense line segments to sparse endpoint proposals to produce initial wireframes, and (3) filtering false positive proposals through a novel endpoint-decoupled line-of-interest aligning (EPD LOIAlign) module that captures the co-occurrence between endpoint proposals and HAT fields for better verification. Thanks to our novel designs, HAWPv2 shows strong performance in fully supervised learning, while HAWPv3 excels in self-supervised learning, achieving superior repeatability scores and efficient training (24 GPU hours on a single GPU). Furthermore, HAWPv3 exhibits a promising potential for wireframe parsing in out-of-distribution images without providing ground truth labels of wireframes.},
	language = {en},
	number = {12},
	urldate = {2025-01-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xue, Nan and Wu, Tianfu and Bai, Song and Wang, Fu-Dong and Xia, Gui-Song and Zhang, Liangpei and Torr, Philip H. S.},
	month = dec,
	year = {2023},
	note = {arXiv:2210.12971 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {14727--14744},
	annote = {Comment: Journal extension of arXiv:2003.01663; Accepted by IEEE TPAMI; Code is available at https://github.com/cherubicxn/hawp},
	file = {PDF:/home/ziegler/Zotero/storage/E9QMD9KV/Xue et al. - 2023 - Holistically-Attracted Wireframe Parsing From Supervised to Self-Supervised Learning.pdf:application/pdf},
}

@inproceedings{pautrat_deeplsd_2023,
	address = {Vancouver, BC, Canada},
	title = {{DeepLSD}: {Line} {Segment} {Detection} and {Refinement} with {Deep} {Image} {Gradients}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0129-8},
	shorttitle = {{DeepLSD}},
	url = {https://ieeexplore.ieee.org/document/10204382/},
	doi = {10.1109/CVPR52729.2023.01662},
	abstract = {Line segments are ubiquitous in our human-made world and are increasingly used in vision tasks. They are complementary to feature points thanks to their spatial extent and the structural information they provide. Traditional line detectors based on the image gradient are extremely fast and accurate, but lack robustness in noisy images and challenging conditions. Their learned counterparts are more repeatable and can handle challenging images, but at the cost of a lower accuracy and a bias towards wireframe lines. We propose to combine traditional and learned approaches to get the best of both worlds: an accurate and robust line detector that can be trained in the wild without ground truth lines. Our new line segment detector, DeepLSD, processes images with a deep network to generate a line attraction field, before converting it to a surrogate image gradient magnitude and angle, which is then fed to any existing handcrafted line detector. Additionally, we propose a new optimization tool to refine line segments based on the attraction field and vanishing points. This refinement improves the accuracy of current deep detectors by a large margin. We demonstrate the performance of our method on low-level line detection metrics, as well as on several downstream tasks using multiple challenging datasets. The source code and models are available at https://github.com/cvg/DeepLSD.},
	language = {en},
	urldate = {2025-01-10},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pautrat, Rémi and Barath, Daniel and Larsson, Viktor and Oswald, Martin R. and Pollefeys, Marc},
	month = jun,
	year = {2023},
	pages = {17327--17336},
	file = {PDF:/home/ziegler/Zotero/storage/JUI36W67/Pautrat et al. - 2023 - DeepLSD Line Segment Detection and Refinement with Deep Image Gradients.pdf:application/pdf},
}

@misc{casanova_reinforced_2020,
	title = {Reinforced active learning for image segmentation},
	url = {http://arxiv.org/abs/2002.06583},
	doi = {10.48550/arXiv.2002.06583},
	abstract = {Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions – opposed to entire images – to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modiﬁcation of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30\% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we ﬁnd that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Casanova, Arantxa and Pinheiro, Pedro O. and Rostamzadeh, Negar and Pal, Christopher J.},
	month = feb,
	year = {2020},
	note = {arXiv:2002.06583 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICLR2020},
	file = {PDF:/home/ziegler/Zotero/storage/U6LA3QR3/Casanova et al. - 2020 - Reinforced active learning for image segmentation.pdf:application/pdf},
}

@misc{srikishan_reinforcement_2024,
	title = {Reinforcement {Learning} as a {Parsimonious} {Alternative} to {Prediction} {Cascades}: {A} {Case} {Study} on {Image} {Segmentation}},
	shorttitle = {Reinforcement {Learning} as a {Parsimonious} {Alternative} to {Prediction} {Cascades}},
	url = {http://arxiv.org/abs/2402.11760},
	doi = {10.48550/arXiv.2402.11760},
	abstract = {Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such large architectures lead to increased accuracy, this is usually accompanied by a larger increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning (ML) pipelines, the recent confluence of machine learning and fields like the Internet of Things (IoT) has rendered such large architectures infeasible for execution in low-resource settings. For some datasets, large monolithic pipelines may be overkill for simpler inputs. To address this problem, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to sub-optimal throughput and increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an efficient alternative to cascaded decision architectures. Through experimental evaluation on both real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174\% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR’s adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4\% over SOTA models. Code and data are available at https://github.com/scailab/paser.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Srikishan, Bharat and Tabassum, Anika and Allu, Srikanth and Kannan, Ramakrishnan and Muralidhar, Nikhil},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11760 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/home/ziegler/Zotero/storage/T6CBU8SE/Srikishan et al. - 2024 - Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades A Case Study on Image S.pdf:application/pdf},
}

@misc{wang_outline_2018,
	title = {Outline {Objects} using {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1804.04603},
	doi = {10.48550/arXiv.1804.04603},
	abstract = {Image segmentation needs both local boundary position information and global object context information. The performance of the recent state-of-the-art method, fully convolutional networks, reaches a bottleneck due to the neural network limit after balancing between the two types of information simultaneously in an end-to-end training style. To overcome this problem, we divide the semantic image segmentation into temporal subtasks. First, we ﬁnd a possible pixel position of some object boundary; then trace the boundary at steps within a limited length until the whole object is outlined. We present the ﬁrst deep reinforcement learning approach to semantic image segmentation, called DeepOutline, which outperforms other algorithms in Coco detection leaderboard in the middle and large size person category in Coco val2017 dataset. Meanwhile, it provides an insight into a divide and conquer way by reinforcement learning on computer vision problems.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Wang, Zhenxin and Sarcar, Sayan and Liu, Jingxin and Zheng, Yilin and Ren, Xiangshi},
	month = apr,
	year = {2018},
	note = {arXiv:1804.04603 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/ziegler/Zotero/storage/8QQIXSRW/Wang et al. - 2018 - Outline Objects using Deep Reinforcement Learning.pdf:application/pdf},
}

@misc{jie_tree-structured_2017,
	title = {Tree-{Structured} {Reinforcement} {Learning} for {Sequential} {Object} {Localization}},
	url = {http://arxiv.org/abs/1703.02710},
	doi = {10.48550/arXiv.1703.02710},
	abstract = {Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reﬂects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to ﬁnd multiple objects with a single feedforward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Jie, Zequn and Liang, Xiaodan and Feng, Jiashi and Jin, Xiaojie and Lu, Wen Feng and Yan, Shuicheng},
	month = mar,
	year = {2017},
	note = {arXiv:1703.02710 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Advances in Neural Information Processing Systems 2016},
	file = {PDF:/home/ziegler/Zotero/storage/KDH9WN3U/Jie et al. - 2017 - Tree-Structured Reinforcement Learning for Sequential Object Localization.pdf:application/pdf},
}

@misc{florensa_reverse_2018,
	title = {Reverse {Curriculum} {Generation} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1707.05300},
	doi = {10.48550/arXiv.1707.05300},
	abstract = {Many relevant tasks require an agent to reach a certain state or to manipulate objects into a desired conﬁguration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-speciﬁc reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in “reverse,” gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent’s performance, leading to efﬁcient training on goal-oriented tasks. We demonstrate our approach on difﬁcult simulated navigation and ﬁne-grained manipulation problems, not solvable by state-ofthe-art reinforcement learning methods.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
	month = jul,
	year = {2018},
	note = {arXiv:1707.05300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	annote = {Comment: Published at the 1st Conference on Robot Learning (CoRL 2017)},
	file = {PDF:/home/ziegler/Zotero/storage/6FRVDDPC/Florensa et al. - 2018 - Reverse Curriculum Generation for Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{bengio-curriculum,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}